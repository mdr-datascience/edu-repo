{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malware Detection in Network Traffic\n",
    "\n",
    "## Random Forest Classification\n",
    "\n",
    "Run the quickstart_ml_eda.ipynb to obtain the preprocessed data for this analysis.\n",
    "\n",
    "This tutorial is based on the following work:\n",
    "\n",
    "https://github.com/aruberts/tutorials/blob/main/pyspark/spark_feature_engineering.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=TlXqsL4ysB0&t=1322s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"iot\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed Parquet data\n",
    "\n",
    "Load data preprocessed in the previous tutorial and while loading create new variable \"is_bad\" with the numerical value 1 for malicious traffic and 0 for benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = spark.read.parquet(\"/content/drive/MyDrive/NetworkMalwareData/processed.pq\").withColumn(\n",
    "    \"is_bad\", F.when(F.col(\"label\") != \"Benign\", 1).otherwise(0)\n",
    ")\n",
    "df_fe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "]\n",
    "categorical_features = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "categorical_features_indexed = [c + \"_index\" for c in categorical_features]\n",
    "\n",
    "input_features = numerical_features + categorical_features_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_valid_values = {}\n",
    "\n",
    "for c in categorical_features:\n",
    "    # Find frequent values\n",
    "    categorical_valid_values[c] = (\n",
    "        df_fe.groupby(c)\n",
    "        .count()\n",
    "        .filter(F.col(\"count\") > 100)\n",
    "        .select(c)\n",
    "        .toPandas()\n",
    "        .values.ravel()\n",
    "    )\n",
    "\n",
    "    df_fe = df_fe.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c).isin(list(categorical_valid_values[c])), F.col(c)).otherwise(\n",
    "            F.lit(\"Other\").alias(c)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Train test split will need to be done using the source IP address, otherwise we risk leaking data. The best way to do this is by splitting the IP addresses at random, and then filtering the data frame according to the IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.groupby(\"source_ip\").agg(F.sum(F.col(\"is_bad\")).alias(\"bad_sum\")).orderBy(\"bad_sum\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training non-malicious IPs (80%)\n",
    "train_ips = (\n",
    "    df_fe.where(\n",
    "        ~F.col(\"source_ip\").isin([\"192.168.100.103\", \"192.168.2.5\", \"192.168.2.1\"])\n",
    "    )\n",
    "    .select(F.col(\"source_ip\"), F.lit(1).alias(\"is_train\"))\n",
    "    .dropDuplicates()\n",
    "    .sample(0.8)\n",
    ")\n",
    "\n",
    "\n",
    "df_fe = df_fe.join(train_ips, \"source_ip\", \"left\")\n",
    "\n",
    "# Add 1 malicious IP to training and testing data\n",
    "df_train = df_fe.where((F.col(\"is_train\") == 1) | (F.col(\"source_ip\") == \"192.168.100.103\"))\n",
    "df_test = df_fe.where((F.col(\"is_train\") != 1) | (F.col(\"source_ip\") == \"192.168.2.5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = StringIndexer(inputCols=categorical_features, outputCols=categorical_features_indexed, handleInvalid='skip')\n",
    "va = VectorAssembler(inputCols=input_features, outputCol=\"features\", handleInvalid='skip' )\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_bad\", numTrees=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[ind, va, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(df_train)\n",
    "test_preds = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderROC\")\n",
    "print(\"ROC AUC\", roc.evaluate(test_preds))\n",
    "\n",
    "pr = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderPR\")\n",
    "print(\"PR AUC\", pr.evaluate(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"importance\": list(pipeline.stages[-1].featureImportances),\n",
    "        \"feature\": pipeline.stages[-2].getInputCols(),\n",
    "    }\n",
    ").sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stages[-1].write().overwrite().save(\"rf_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write().overwrite().save(\"pipeline_basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
