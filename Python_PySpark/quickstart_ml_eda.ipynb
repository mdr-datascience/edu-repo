{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malware Detection in Network Traffic\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import pyspark functions individually, or all functions into a single variable (F) and the use F.function to call individual functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_unixtime,\n",
    "    to_timestamp,\n",
    "    min,\n",
    "    max,\n",
    "    sum,\n",
    "    avg,\n",
    "    col,\n",
    "    countDistinct,\n",
    "    broadcast,\n",
    "    date_trunc,\n",
    "    count,\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import plotly.express as px\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"malware\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files\n",
    "Read data files. Here two files are chosen to illustrate that pyspark can read multiple files in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\"CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\", \"CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv\"]\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \"|\").csv(filepaths, inferSchema = True, header = True)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by converting the ts variable from unix time to a timestamp format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dt\", from_unixtime(\"ts\")).withColumn(\"dt\", to_timestamp(\"dt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename some columns for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnsRenamed(\n",
    "    {\n",
    "        \"id.orig_h\": \"source_ip\",\n",
    "        \"id.orig_p\": \"source_port\",\n",
    "        \"id.resp_h\": \"dest_ip\",\n",
    "        \"id.resp_p\": \"dest_port\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check min and max values of the connection time variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(\n",
    "    min(\"dt\").alias(\"min_date\"), \n",
    "    max(\"dt\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the data frame, number of rows are the observations, and columns are features (and labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose columns to analyse and show unique counts per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyse = [\n",
    "    \"source_ip\",\n",
    "    \"source_port\",\n",
    "    \"dest_ip\",\n",
    "    \"dest_port\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]\n",
    "\n",
    "unique_counts = df.agg(*(countDistinct(col(c)).alias(c) for c in to_analyse))\n",
    "print(unique_counts.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop statitc columns (with a single unique observation throughout the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = unique_counts.first()\n",
    "static_cols = [c for c in unique_counts.asDict() if unique_counts[c] == 1]\n",
    "print(\"Dataset has\", len(static_cols), \"static columns: \", static_cols)\n",
    "df = df.drop(*static_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unique source and destination IP addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ips = df.select(col(\"source_ip\")).distinct()\n",
    "dest_ips = df.select(col(\"dest_ip\")).distinct()\n",
    "common_ips = source_ips.join(broadcast(dest_ips), source_ips.source_ip == dest_ips.dest_ip, how='inner')\n",
    "\n",
    "\n",
    "print(\"Source IPs count:\", source_ips.count())\n",
    "print(\"Dest IPs count:\", dest_ips.count())\n",
    "print(\"IPs as both:\", common_ips.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unique source and destination ports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ports = df.select(col(\"source_port\")).distinct()\n",
    "dest_ports = df.select(col(\"dest_port\")).distinct()\n",
    "common_ports = source_ports.join(broadcast(dest_ports), source_ports.source_port == dest_ports.dest_port, how='inner')\n",
    "\n",
    "\n",
    "print(\"Source Ports count:\", source_ports.count())\n",
    "print(\"Dest Ports count:\", dest_ports.count())\n",
    "print(\"Ports as both:\", common_ports.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count missing data by first replacing the missing data entries, in this case defined as \"-\", with Null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\"-\", None)\n",
    "\n",
    "remaining_cols = [f for f in to_analyse if f not in static_cols]\n",
    "df.select(\n",
    "    [count(F.when(F.isnan(c) | col(c).isNull(), c)).alias(c) for c in remaining_cols]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot time-series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot data across time, we can first create 4 new columns that correspond to the day, hour, minute and second of the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns(\n",
    "    {\n",
    "        \"day\": date_trunc(\"day\", \"dt\"),\n",
    "        \"hour\": date_trunc(\"hour\", \"dt\"),\n",
    "        \"minute\": date_trunc(\"minute\", \"dt\"),\n",
    "        \"second\": date_trunc(\"second\", \"dt\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series of the number of benign and malicious labels in the dataset per minute, hour and daY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agg in ['day', 'hour', 'minute']:\n",
    "    plotting_table = df.groupBy([agg, \"label\"]).agg(count(\"uid\").alias(\"counts\")).orderBy(agg).toPandas()\n",
    "    fig = px.line(plotting_table, x=agg, y=\"counts\", color=\"label\", title=f'Event Counts per {agg}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make bar plots of five categorical variables in the dataset (proto, service, conn_state, history and label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(df, var):\n",
    "    var_counts = df.groupBy(var).count().orderBy(\"count\", ascending=False)\n",
    "    var_counts = var_counts.withColumn(\n",
    "        \"percent\", F.round(col(\"count\") / sum(col(\"count\")).over(Window.partitionBy()), 4)\n",
    "    )\n",
    "    var_counts.show()\n",
    "    fig = px.bar(var_counts.toPandas(), x=var, y=\"count\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "categorical_columns = [\"proto\", \"service\", \"conn_state\", \"history\", \"label\"]\n",
    "\n",
    "for c in categorical_columns:\n",
    "    counts(df, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further processing the data for ML analysis, start by identifying the categorical and numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "]\n",
    "categorical_cols = [\"proto\", \"service\", \"conn_state\"]\n",
    "label = \"label\"\n",
    "\n",
    "all_cols = numerical_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical data, replace the Null values by -999999 (which is recognized as Null data by the ML algorithm) and for the categorical data, replace the Nulls by \"missing\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recast_cols = {}\n",
    "fill_vals = {}\n",
    "for c in numerical_cols:\n",
    "    recast_cols[c] = col(c).cast(\"double\")\n",
    "    fill_vals[c] = -999999\n",
    "\n",
    "for c in categorical_cols:\n",
    "    fill_vals[c] = 'missing'\n",
    "    \n",
    "df = df.withColumns(recast_cols)\n",
    "df = df.fillna(fill_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show sample of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline\n",
    "Finally the following cell includes all the preprocessing done in this notebook but organised in a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\"local_orig\", \"local_resp\", \"missed_bytes\", \"tunnel_parents\"]\n",
    "\n",
    "recast_cols = {\n",
    "    \"duration\": col(\"duration\").cast(\"double\"),\n",
    "    \"orig_bytes\": col(\"orig_bytes\").cast(\"double\"),\n",
    "    \"resp_bytes\": col(\"resp_bytes\").cast(\"double\"),\n",
    "    \"orig_ip_bytes\": col(\"orig_ip_bytes\").cast(\"double\"),\n",
    "    \"orig_pkts\": col(\"orig_pkts\").cast(\"double\"),\n",
    "    \"resp_pkts\": col(\"resp_pkts\").cast(\"double\"),\n",
    "    \"resp_ip_bytes\": col(\"resp_ip_bytes\").cast(\"double\"),\n",
    "}\n",
    "\n",
    "fill_vals = {\n",
    "    \"duration\": -999999,\n",
    "    \"orig_bytes\": -999999,\n",
    "    \"resp_bytes\": -999999,\n",
    "    \"orig_pkts\": -999999,\n",
    "    \"orig_ip_bytes\": -999999,\n",
    "    \"resp_pkts\": -999999,\n",
    "    \"resp_ip_bytes\": -999999,\n",
    "    \"history\": \"missing\",\n",
    "    \"proto\": \"missing\",\n",
    "    \"service\": \"missing\",\n",
    "    \"conn_state\": \"missing\",\n",
    "}\n",
    "\n",
    "preprocessed_data = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .csv(filepaths, inferSchema=True, header=True)\n",
    "    .withColumn(\"dt\", to_timestamp(from_unixtime(\"ts\")))\n",
    "    .withColumns(\n",
    "        {\n",
    "            \"day\": date_trunc(\"day\", \"dt\"),\n",
    "            \"hour\": date_trunc(\"hour\", \"dt\"),\n",
    "            \"minute\": date_trunc(\"minute\", \"dt\"),\n",
    "            \"second\": date_trunc(\"second\", \"dt\"),\n",
    "        }\n",
    "    )\n",
    "    .withColumnsRenamed(\n",
    "        {\n",
    "            \"id.orig_h\": \"source_ip\",\n",
    "            \"id.orig_p\": \"source_port\",\n",
    "            \"id.resp_h\": \"dest_ip\",\n",
    "            \"id.resp_p\": \"dest_port\",\n",
    "        }\n",
    "    )\n",
    "    .drop(*static_cols)\n",
    "    .replace(\"-\", None)\n",
    "    .withColumns(recast_cols)\n",
    "    .fillna(fill_vals)\n",
    ")\n",
    "\n",
    "preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sava preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and load the preprocessed data as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.write.parquet(\"processed.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in = spark.read.parquet(\"processed.pq\")\n",
    "read_in.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
