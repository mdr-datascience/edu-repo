{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24aa5c7c",
   "metadata": {},
   "source": [
    "# Tutorial 1: Spark's Machine Learning library (MLlib)\n",
    "## Machine Learning using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a444a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddda9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f595f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2533c7",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b95987",
   "metadata": {},
   "source": [
    "Create data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8900c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb007e",
   "metadata": {},
   "source": [
    "Calculate correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec446",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6129d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a5ed1",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Create a new dataset using sparse vectors. Calculate the correlation between features using both numpy and pyspark, the output should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481db21",
   "metadata": {},
   "source": [
    "# Summarizer (mean, sum, variance, std, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# Create summarizer for multiple metrics \"mean\", \"count\", etc.\n",
    "summarizer = Summarizer.metrics(\"mean\")\n",
    "\n",
    "# Compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
    "\n",
    "# Alternative way (use for more than one metric)\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc531ac8",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Calculate and display multiple metrics using summarizer.summary(), for example calculate the min, max, mean and std. For more metrics please see here:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.stat.Summarizer.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0fcfa",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f359261",
   "metadata": {},
   "source": [
    "Create toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e199ed2",
   "metadata": {},
   "source": [
    "Create logistic regression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf35567",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model = lr.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e68cb",
   "metadata": {},
   "source": [
    "Create test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d82f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed14dd",
   "metadata": {},
   "source": [
    "Make predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "prediction = model.transform(test)\n",
    "result = prediction.select(\"features\", \"label\", \"probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322546d",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Try training logistic regression on the iris dataset from scikit learn:\n",
    "1. Import scikit-learn and load iris dataset.\n",
    "2. Divide into train and test datasets.\n",
    "3. Import pandas and create dataframes for train and test data (include a column called 'label' with the labels)\n",
    "4. Create PySpark's dataframes from the Pandas' dataframes.\n",
    "5. Use VectorAssembler to create features column.\n",
    "6. Follow the rest of the logistic regression example to complete the task.\n",
    "\n",
    "Use the following code to create the VectorAssembler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd1b2a",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d3673",
   "metadata": {},
   "source": [
    "Create text dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca046f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4933697",
   "metadata": {},
   "source": [
    "Create Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8737361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da84995",
   "metadata": {},
   "source": [
    "Create test dataset and test Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8857b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2771ce8",
   "metadata": {},
   "source": [
    "# Cross-Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdfcde",
   "metadata": {},
   "source": [
    "Create new dataset and Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc0461",
   "metadata": {},
   "source": [
    "Use cross-validation to train the model and tune its hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45252a3c",
   "metadata": {},
   "source": [
    "Test tuned model on new test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf292c9",
   "metadata": {},
   "source": [
    "# Train-Validation Split and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53525b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/content/drive/MyDrive/NetworkMalwareData/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbffa37",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "\n",
    "Let's work on the iris dataset again from scikit learn. Train a ML algorithm on this dataset while tuning its hyperparameters on valiation data either using the CrossValidator functionality or TrainValidationSplit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fceda",
   "metadata": {},
   "source": [
    "# Use StandardScaler to scale the numerical/vector features\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
